# Taller: Regresión Lineal

## 2. Masa Corporal y Varios Diámetros

Lo primero a realizar está en cargar el dataset a trabajar. Este dataset está disponible [paquí](http//www.stpatsci.org/data/oz/physical.txt).

```{r}
pdata <- read.table("http://www.statsci.org/data/oz/physical.txt", sep = "\t", header = TRUE)
```
```{r, echo=FALSE}
knitr::kable(pdata[c(1:5), ])
```

Ya con los datos cargados, debemos crear el modelo de la regresión multiple. Como se busca realizar predicciones sobre el parámetro de masa a partir de las medidas tomadas, emplearemos la función `lm` para poder realizar planteamiento del modelo.


```{r}
fml <- pdata$Mass ~ pdata$Fore + pdata$Bicep + pdata$Chest + pdata$Neck + pdata$Shoulder + pdata$Waist + pdata$Height + pdata$Calf + pdata$Thigh + pdata$Head

flm <- lm(formula = fml)
```

Ya tras pasar nuestra formula a los la función lm, podremos extraer cada uno de los coeficientes para cada uno de los parámetros correspondientes.

```{r}
sumLM <- summary(flm)
vecCoef <- sumLM$coefficients[c(1:11)]
```

```{r, echo = FALSE}
vecCoef
```

A partir de estos coeficientes, tendremos nuestro modelo para realizar la regresión lineal de la masa corporal.

\begin{equation}
    Mass = -69.517 +  1.781 \cdot Fore + 0.155 \cdot Bicep + 0.189 \cdot Chest -0.481 \cdot Neck -0.0293 \cdot Shoulder 
\end{equation}
    
\begin{equation}
    + 0.661 \cdot Waist + 0.317 \cdot Height + 0.445 \cdot Calf + 0.297 \cdot Thigh -0.919 \cdot Head
\end{equation}

Ya teniendo estos valores creados, es cuestión de pasar estos a una función la cual podamos usar para predecir los valores de la masa corporal.

```{r}
massReg <- function(diamData) {
    predVal <- vecCoef[1]

    for (i in c(1:length(diamData))) {
        predVal <- predVal + vecCoef[1 + i] * diamData[i]
    }

    return(predVal)
}
```

Ahora, a partir de esto, podemos realizar el desarrollo de los diferentes literales.

### a. Residuo Ajustado

En este caso, el realizar el cálculo del residuo de manera normal sería especialmente complicado, esto se debe principalmente a la gran cantidad de variables independientes que se están trabajando dentro de esta regresión. Afortunadamente, tenemos la posibilidad de usar nuevamente la función `lm` para poder realizar el cálculo de esto.

```{r}
vecResi <- sumLM$residual
```
```{r, echo=FALSE}
vecResi
```

A partir de esto, podemos realizar la gráfica de los residuos para cada uno de los puntos dentro de la vinculación.

```{r, echo = FALSE, fig.align='center'}
plot(pdata$Mass, vecResi, ylab = "Residual", xlab = "Masa Corporal", main = "Residuo de la regresión lineal")
abline(h = 0)
segments(pdata$Mass, 0, pdata$Mass, vecResi)
```

Siendo así, podemos ver que la regresión lineal realizada se ajusta de manera significativa a los datos que empleamos a manera de prueba. Cabe resaltar que, aunque existen registros relativamente alejados de la línea central, vemos que, aparentemente, la gran mayoría se encuentran a menos de 3 unidades de distancia.

### b. Residuo ajustado de la raíz cúbica

En esencia, tenemos que repetir el mismo proceso realizado para la determinación de la gráfica de residuo del literal anterior, alterando la ecuación que le pasamos a la función `lm` como parámetro.

```{r}
fmls <- pdata$Mass**(1 / 3) ~ pdata$Fore + pdata$Bicep + pdata$Chest + pdata$Neck + pdata$Shoulder + pdata$Waist + pdata$Height + pdata$Calf + pdata$Thigh + pdata$Head

sumLMs <- summary(lm(formula = fmls))
vecResis <- sumLMs$residual
```
```{r, echo=FALSE}
vecResis
```

Ya con estos valores calculados, podemos realizar la gráfica correspondiente.

```{r, echo=FALSE, fig.align='center'}
plot(pdata$Mass, vecResis, ylab = "Residual", xlab = "Masa Corporal", main = "Residuo de la regresión lineal de la raíz cúbica")
abline(h = 0)
segments(pdata$Mass, 0, pdata$Mass, vecResis)
```

Como podemos ver, en este caso, la regresión lineal de este caso nos da como resultado con un residuo realmente cercano a línea recta en el cual ningún valor supera el 0.08 de distancia respecto a la línea central.

### c. Comparación de los Resultados

Con el fin de determinar cual de las 2 regresiones es la mejor de las 2, se emplearán boxplots los cuales nos permitirán el realizar la comparación de la una manera relativamente directa entre cada una de estas.

```{r, echo=FALSE, fig.align='center'}
par(mfrow = c(1, 2))
boxplot(vecResi, main = "Boxplot: Residuo")
boxplot(vecResis, main = "Boxplot: Residuo Raíz")
```

Entonces, como podemos ver, y lo más notable, está en que la escala que se está trabajando son comparablemente diferentes. La gráfica del residuo está trabajando en términos de unidades mientras que la de raíz está trabajando con centecimales. De igual, manera, como es apreciable, los _bigotes_ de la caja en el caso de la regresión de la raíz, son más compactos en comparación con los del caso normal. 

En conclusión, podemos decir que la regresión lineal realizado para el caso de la raíz cúbica, es, de manera compativa, mucho mejor que nuestro caso base.